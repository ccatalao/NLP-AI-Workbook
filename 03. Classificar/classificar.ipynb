{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINGUAGEM NATURAL E INTELIGÊNCIA ARTIFICIAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICAÇÃO AUTOMÁTICA DE TEXTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Catalão Alves  \n",
    "13 Maio, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificar textos de acções de divulgação científica, com aplicação de aprendizagem automática supervisionada.  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os textos utilizados nesta aplicação são obtidos a partir de dois ficheiros em formato CSV:  \n",
    "\n",
    "1. textos_treino.csv (com textos classificados, para treino do modelo)   \n",
    "2. textos_classificar.csv (com os novos textos para classificação automática) \n",
    "\n",
    "O resultado final será a criação de dois ficheiros com os novos textos já classificados,   \n",
    "com indicação da respectiva probabilidade:\n",
    "\n",
    "1. textos_classificados.csv (ficheiro csv, delimitado com tabs)  \n",
    "2. textos_classifcados.xlsx (ficheiro excel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livrarias python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livrarias NLTK - Natural Langage Processing Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livrarias SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "novo_corpus = []\n",
    "categorias = [ \"outra\",\"Astronomia\",\"Biologia\",\"Geologia\",\"Engenharia\",\"Patrimonio\"]\n",
    "\n",
    "TREINO = \"textos_treino.csv\"\n",
    "CLASSIFICAR = \"textos_classificar.csv\"\n",
    "RESULTADOS = \"textos_classificados\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizar(texto, stemizar=True, lingua=\"portuguese\"):\n",
    "\n",
    "    # Usa o SnowballStemmer do NLTK para português\n",
    "    stemizador = SnowballStemmer(lingua)\n",
    "\n",
    "    # Tokeniza por frase e por palavra\n",
    "    tokens = [word.lower() for frase in nltk.sent_tokenize(texto) for word in nltk.word_tokenize(frase)]\n",
    "    \n",
    "    # Aplica NLTK stopwords \n",
    "    stop_words_pt = set(stopwords.words(lingua)) \n",
    "    \n",
    "    # Filtra as palavras que não têm letras, e as que estão na lista de stopwords\n",
    "    tokens_filtrados = []\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            if token not in stopwords.words(lingua):\n",
    "                if token not in stop_words_pt:\n",
    "                    tokens_filtrados.append(token)\n",
    "                if stemizar:\n",
    "                    stems = [stemizador.stem(token) for token in tokens_filtrados]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textos para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importando os dados de treino\n",
    "dados_treino = pd.read_csv(TREINO, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos textos de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A treinar com textos classificados ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nA treinar com textos classificados ...\\n\")\n",
    "for i in range(len(dados_treino)):    \n",
    "    termos = tokenizar(dados_treino[\"texto\"][i], stemizar=True, lingua=\"portuguese\")    \n",
    "    # refaz cada linha com os tokens processados\n",
    "    termos = ' '.join(termos)\n",
    "    corpus.append(termos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcular a matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorizar só as 100 palavras mais frequentes\n",
    "cv = CountVectorizer(max_features = 100)\n",
    "\n",
    "# Obter a variável independente X (ie., os textos já classificados)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "# Obter a variável independente y (a classificação dos textos)\n",
    "# \":\" todas as linhas, e \"2\" a coluna com a classific\n",
    "y = dados_treino.iloc[:, 2].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicar o modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplicar o modelo Multinominal Naive Bayes\n",
    "clf = MultinomialNB().fit(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar novos textos para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dados_classificar = pd.read_csv(CLASSIFICAR, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos novos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classificar novos textos ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"A classificar novos textos ...\\n\")\n",
    "for i in range(len(dados_classificar)):    \n",
    "    termos = tokenizar(dados_classificar[\"texto\"][i], stemizar=True, lingua=\"portuguese\")    \n",
    "    # refaz cada linha com os termos processados\n",
    "    termos = ' '.join(termos)\n",
    "    novo_corpus.append(termos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcular a matiz TF-IDF dos novos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_novo = cv.transform(novo_corpus)\n",
    "X_novo_tfidf = tfidf_transformer.transform(X_novo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previsão de classificações e respectivas probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "previsoes = clf.predict(X_novo_tfidf)\n",
    "probabilidades = clf.predict_proba(X_novo_tfidf)\n",
    "\n",
    "nova_classificacao = []\n",
    "for i in range(len(dados_classificar)): \n",
    "    p = previsoes[i]\n",
    "    #pb = p - 1 # Obter a coluna com a probabilidade válida\n",
    "\n",
    "    line = {\n",
    "            \"titulo\":dados_classificar[\"titulo\"][i],\n",
    "            \"texto\":dados_classificar[\"texto\"][i],\n",
    "            \"previsao\":p,\n",
    "            \"categoria\":categorias[previsoes[i]],\n",
    "            #p - 1 para a coluna com a probabilidade válida\n",
    "            \"probabilidade\":(probabilidades[i][p-1]) * 100,             \n",
    "            }\n",
    "    OrderedDict(line) # para preservar a ordem das colunas\n",
    "    nova_classificacao.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar os novos textos, já classificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados gravados em excel e csv.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar o dataframe com os textos classificados\n",
    "df = pd.DataFrame(nova_classificacao)\n",
    "# Manter a ordem das colunas no dataframe \n",
    "df = df[[ k for k in nova_classificacao[0].keys()]] \n",
    "\n",
    "# Salvar num ficheiro CSV\n",
    "df.to_csv(RESULTADOS + \".csv\", index=False, sep='\\t')\n",
    "\n",
    "# Salvar num ficheiro excel\n",
    "df.to_excel(RESULTADOS + \".xlsx\", index=False)\n",
    "\n",
    "print(\"Resultados gravados em excel e csv.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
